# Przewodnik: Robot Humanoidalny Unitree G1 EDU-U6 w MuJoCo Playground

## Spis treÅ›ci
1. [WstÄ™p](#wstÄ™p)
2. [Czym jest MuJoCo Playground](#czym-jest-mujoco-playground)
3. [Robot Unitree G1 - Specyfikacja](#robot-unitree-g1---specyfikacja)
4. [Instalacja i konfiguracja](#instalacja-i-konfiguracja)
5. [Pierwsze kroki - Symulacja G1](#pierwsze-kroki---symulacja-g1)
6. [Trening polityki sterowania](#trening-polityki-sterowania)
7. [Praktyczne przykÅ‚ady](#praktyczne-przykÅ‚ady)
8. [Analiza i debugowanie](#analiza-i-debugowanie)
9. [Transfer sim-to-real](#transfer-sim-to-real)
10. [CzÄ™sto zadawane pytania](#czÄ™sto-zadawane-pytania)

---

## WstÄ™p

Ten przewodnik zostaÅ‚ przygotowany specjalnie dla studentÃ³w Politechniki Rzeszowskiej pracujÄ…cych nad projektem z robotem humanoidalnym **Unitree G1 EDU-U6**. Celem jest umoÅ¼liwienie szybkiego opanowania narzÄ™dzi do symulacji i uczenia robotÃ³w przed przejÅ›ciem do pracy z fizycznym sprzÄ™tem.

### Dlaczego symulacja?

- **BezpieczeÅ„stwo**: MoÅ¼esz eksperymentowaÄ‡ bez ryzyka uszkodzenia drogiego sprzÄ™tu
- **SzybkoÅ›Ä‡**: Symulacje na GPU sÄ… tysiÄ…ce razy szybsze niÅ¼ czas rzeczywisty
- **PowtarzalnoÅ›Ä‡**: MoÅ¼esz Å‚atwo powtarzaÄ‡ eksperymenty z identycznymi warunkami
- **Koszt**: Nie potrzebujesz fizycznego robota do nauki i eksperymentÃ³w

---

## Czym jest MuJoCo Playground

MuJoCo Playground to platforma do:
- **Symulacji robotÃ³w** z wykorzystaniem silnika fizyki MuJoCo
- **Uczenia ze wzmocnieniem (Reinforcement Learning)** z akceleracjÄ… GPU
- **Trenowania polityk sterowania** w rÃ³wnolegÅ‚ych Å›rodowiskach
- **Transferu sim-to-real** - przenoszenia polityk z symulacji do rzeczywistoÅ›ci

### Kluczowe komponenty:

1. **MuJoCo** - silnik fizyki symulujÄ…cy dynamikÄ™ robotÃ³w
2. **MJX (MuJoCo JAX)** - wersja MuJoCo zoptymalizowana dla GPU
3. **JAX** - framework do obliczeÅ„ numerycznych z automatycznym rÃ³Å¼niczkowaniem
4. **PPO** - algorytm uczenia ze wzmocnieniem (Proximal Policy Optimization)

---

## Robot Unitree G1 - Specyfikacja

### Charakterystyka robota G1:

- **Typ**: Humanoid (robot dwunoÅ¼ny, dwurÄ™czny)
- **WysokoÅ›Ä‡**: ~130 cm
- **Waga**: ~35 kg
- **Stopnie swobody**: 23 DOF (Degrees of Freedom)
  - Nogi: 12 DOF (po 6 na kaÅ¼dÄ… nogÄ™)
  - TuÅ‚Ã³w: 3 DOF
  - RÄ™ce: 8 DOF (po 4 na kaÅ¼dÄ… rÄ™kÄ™)

### DostÄ™pne Å›rodowiska dla G1:

```python
from mujoco_playground import registry

# Lista wszystkich Å›rodowisk G1
g1_envs = [env for env in registry.ALL_ENVS if 'G1' in env]
print(g1_envs)
# ['G1JoystickFlatTerrain', 'G1InplaceGaitTracking', ...]
```

NajwaÅ¼niejsze Å›rodowiska:
- **G1JoystickFlatTerrain**: Chodzenie po pÅ‚askim terenie z kontrolÄ… joysticka
- **G1InplaceGaitTracking**: Åšledzenie wzorcÃ³w chodu w miejscu
- **G1FlatTerrain**: Podstawowe chodzenie do przodu

---

## Instalacja i konfiguracja

### Wymagania systemowe:

- **System**: Linux (Ubuntu 20.04+ zalecany) lub macOS
- **GPU**: NVIDIA z CUDA 12.x (zalecane dla szybkiego treningu)
- **RAM**: Minimum 16 GB (32 GB zalecane dla duÅ¼ych symulacji)
- **Python**: 3.10 lub nowszy

### Kroki instalacji:

```bash
# 1. Sklonuj repozytorium
git clone git@github.com:google-deepmind/mujoco_playground.git
cd mujoco_playground

# 2. Zainstaluj uv (szybka alternatywa dla pip)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 3. StwÃ³rz Å›rodowisko wirtualne
uv venv --python 3.12
source .venv/bin/activate

# 4. Zainstaluj JAX z obsÅ‚ugÄ… CUDA
uv pip install -U "jax[cuda12]" --index-url https://pypi.org/simple

# 5. SprawdÅº czy GPU jest wykrywane
python -c "import jax; print(f'Backend: {jax.default_backend()}')"
# Powinno wyÅ›wietliÄ‡: Backend: gpu

# 6. Zainstaluj playground ze wszystkimi dodatkami
uv --no-config sync --all-extras

# 7. Zweryfikuj instalacjÄ™
uv --no-config run python -c "import mujoco_playground; print('Sukces!')"

# 8. Pobierz modele robotÃ³w (wÅ‚Ä…cznie z G1)
uv --no-config run python -c "from mujoco_playground import locomotion; locomotion.load('G1JoystickFlatTerrain')"
```

### MoÅ¼liwe problemy i rozwiÄ…zania:

**Problem**: `jax.default_backend()` zwraca 'cpu' zamiast 'gpu'
```bash
# RozwiÄ…zanie: usuÅ„ konfliktujÄ…ce zmienne Å›rodowiskowe
unset LD_LIBRARY_PATH
python -c "import jax; print(jax.default_backend())"
```

**Problem**: Brak CUDA 12
```bash
# SprawdÅº wersjÄ™ CUDA
nvidia-smi
# JeÅ›li masz CUDA 11, zainstaluj JAX dla CUDA 11
uv pip install -U "jax[cuda11_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

---

## Pierwsze kroki - Symulacja G1

### PrzykÅ‚ad 1: Podstawowa symulacja

StwÃ³rz plik `test_g1_basic.py`:

```python
"""
Prosty skrypt do zaÅ‚adowania i wizualizacji robota G1.
Ten przykÅ‚ad pokazuje jak:
1. ZaÅ‚adowaÄ‡ Å›rodowisko G1
2. UruchomiÄ‡ symulacjÄ™
3. ZastosowaÄ‡ losowe akcje
"""

import jax
from mujoco_playground import locomotion

# ZaÅ‚aduj Å›rodowisko G1 na pÅ‚askim terenie z kontrolÄ… joysticka
env = locomotion.load('G1JoystickFlatTerrain')

print(f"Wymiary przestrzeni obserwacji: {env.observation_size}")
print(f"Wymiary przestrzeni akcji: {env.action_size}")

# Zainicjalizuj stan poczÄ…tkowy
rng = jax.random.PRNGKey(0)
state = jax.jit(env.reset)(rng)

print(f"PoczÄ…tkowa pozycja robota: {state.data.qpos[:3]}")  # x, y, z

# Wykonaj 100 krokÃ³w z losowymi akcjami
for i in range(100):
    rng, action_key = jax.random.split(rng)
    # Losowe akcje w zakresie [-1, 1]
    action = jax.random.uniform(action_key, (env.action_size,), minval=-0.1, maxval=0.1)
    state = env.step(state, action)
    
    if i % 20 == 0:
        print(f"Krok {i}: nagroda = {state.reward:.3f}")

print("Symulacja zakoÅ„czona!")
```

Uruchom:
```bash
python test_g1_basic.py
```

### PrzykÅ‚ad 2: Wizualizacja trajektorii

```python
"""
Wizualizacja trajektorii robota G1.
Zapisuje wideo z symulacji.
"""

import jax
import jax.numpy as jp
import mediapy as media
import mujoco
from mujoco_playground import locomotion

env = locomotion.load('G1JoystickFlatTerrain')
rng = jax.random.PRNGKey(42)

# Reset Å›rodowiska
state = jax.jit(env.reset)(rng)

# Lista stanÃ³w do wizualizacji
states = [state]

# Symulacja 200 krokÃ³w z maÅ‚ymi losowymi akcjami
for _ in range(200):
    rng, key = jax.random.split(rng)
    action = jax.random.uniform(key, (env.action_size,), minval=-0.05, maxval=0.05)
    state = env.step(state, action)
    states.append(state)

# Renderowanie wideo
print("Renderowanie wideo...")
frames = env.render(states, height=480, width=640)
media.write_video('g1_simulation.mp4', frames, fps=50)
print("Wideo zapisane jako g1_simulation.mp4")
```

---

## Trening polityki sterowania

### Podstawowy trening

Najprostszy sposÃ³b na rozpoczÄ™cie treningu:

```bash
# Trening na 2 miliony krokÃ³w (ok. 1-2 godziny na dobrej GPU)
python learning/train_jax_ppo.py \
    --env_name G1JoystickFlatTerrain \
    --num_timesteps 2000000 \
    --num_envs 2048
```

### Parametry treningu - co oznaczajÄ…?

```bash
# PrzykÅ‚ad z wyjaÅ›nieniem kaÅ¼dego parametru
# --env_name: Nazwa Å›rodowiska
# --num_timesteps: CaÅ‚kowita liczba krokÃ³w treningu
# --num_envs: Liczba rÃ³wnolegÅ‚ych symulacji
# --num_evals: Ewaluacja co N aktualizacji
# --learning_rate: SzybkoÅ›Ä‡ uczenia
# --entropy_cost: Koszt entropii (eksploracja)
# --batch_size: Rozmiar batcha
# --unroll_length: DÅ‚ugoÅ›Ä‡ sekwencji
# --use_tb: UÅ¼ywaj TensorBoard
# --domain_randomization: WAÅ»NE dla sim-to-real!

python learning/train_jax_ppo.py \
    --env_name G1JoystickFlatTerrain \
    --num_timesteps 5000000 \
    --num_envs 4096 \
    --num_evals 10 \
    --learning_rate 3e-4 \
    --entropy_cost 1e-2 \
    --batch_size 512 \
    --unroll_length 20 \
    --use_tb \
    --domain_randomization
```

### Jak dobraÄ‡ parametry?

**Zbyt wolny trening?**
- ZwiÄ™ksz `--num_envs` (wymaga wiÄ™cej GPU RAM)
- ZwiÄ™ksz `--batch_size`
- Zmniejsz `--num_evals`

**Niestabilny trening?**
- Zmniejsz `--learning_rate` (sprÃ³buj 1e-4)
- Zmniejsz `--batch_size`
- ZwiÄ™ksz `--max_grad_norm`

**Robot uczy siÄ™ za wolno?**
- ZwiÄ™ksz `--num_timesteps`
- Dostosuj funkcjÄ™ nagrody w konfiguracji Å›rodowiska
- SprawdÅº czy domainrandomization nie jest zbyt agresywna

### Kontynuacja treningu

```bash
# WznÃ³w trening z checkpointu
python learning/train_jax_ppo.py \
    --env_name G1JoystickFlatTerrain \
    --load_checkpoint_path logs/G1JoystickFlatTerrain-20250210-120000/checkpoints
```

---

## Praktyczne przykÅ‚ady

### PrzykÅ‚ad 3: Trening z custom rewards

MoÅ¼esz modyfikowaÄ‡ funkcjÄ™ nagrody, aby dostosowaÄ‡ zachowanie robota:

```python
"""
PrzykÅ‚ad modyfikacji konfiguracji nagrÃ³d dla G1.
"""

import json
from mujoco_playground import registry, locomotion

# ZaÅ‚aduj domyÅ›lnÄ… konfiguracjÄ™
config = registry.get_default_config('G1JoystickFlatTerrain')

# Modyfikuj wagi nagrÃ³d
config.reward_config.scales.tracking_lin_vel = 2.0  # ZwiÄ™ksz za podÄ…Å¼anie
config.reward_config.scales.feet_air_time = 3.0     # Nagradzaj dÅ‚uÅ¼szy krok
config.reward_config.scales.orientation = -5.0       # Mocniej karz za przechyÅ‚y

# Zapisz do JSON
overrides = {
    'reward_config': {
        'scales': {
            'tracking_lin_vel': 2.0,
            'feet_air_time': 3.0,
            'orientation': -5.0,
        }
    }
}

# UÅ¼yj w treningu:
# python learning/train_jax_ppo.py \
#   --env_name G1JoystickFlatTerrain \
#   --playground_config_overrides '{"reward_config": {"scales": {"tracking_lin_vel": 2.0}}}'
```

### PrzykÅ‚ad 4: Analiza wytrenowanej polityki

```python
"""
ZaÅ‚aduj wytrenowany model i przeanalizuj jego zachowanie.
"""

import jax
import jax.numpy as jp
from brax.training.agents.ppo import networks as ppo_networks
from mujoco_playground import locomotion
import pickle

# ZaÅ‚aduj Å›rodowisko
env = locomotion.load('G1JoystickFlatTerrain')

# ZaÅ‚aduj checkpoint
checkpoint_path = 'logs/G1JoystickFlatTerrain-20250210-120000/checkpoints/1000000'
with open(f'{checkpoint_path}/params', 'rb') as f:
    params = pickle.load(f)

# StwÃ³rz sieÄ‡ polityki
network = ppo_networks.make_ppo_networks(
    env.observation_size,
    env.action_size,
    preprocess_observations_fn=lambda x: x,
)

# Funkcja inferencji
inference_fn = ppo_networks.make_inference_fn(network)(params, deterministic=True)

# Testuj politykÄ™
rng = jax.random.PRNGKey(0)
state = env.reset(rng)

rewards = []
for i in range(1000):
    action, _ = inference_fn(state.obs, rng)
    state = env.step(state, action)
    rewards.append(state.reward)

print(f"Åšrednia nagroda: {jp.mean(jp.array(rewards)):.3f}")
print(f"CaÅ‚kowita nagroda: {jp.sum(jp.array(rewards)):.3f}")
```

---

## Analiza i debugowanie

### Monitorowanie treningu z TensorBoard

```bash
# Uruchom trening z TensorBoard
python learning/train_jax_ppo.py \
    --env_name G1JoystickFlatTerrain \
    --use_tb

# W osobnym terminalu:
tensorboard --logdir logs/
# OtwÃ³rz http://localhost:6006 w przeglÄ…darce
```

Kluczowe metryki do obserwacji:
- **eval/episode_reward**: Nagroda podczas ewaluacji (cel: powinna rosnÄ…Ä‡)
- **losses/policy_loss**: Strata polityki
- **losses/value_loss**: Strata funkcji wartoÅ›ci
- **losses/total_loss**: CaÅ‚kowita strata

### Debugowanie problemÃ³w

**Robot siÄ™ przewraca:**
1. SprawdÅº skalÄ™ nagrÃ³d dla `orientation` i `base_height`
2. ZwiÄ™ksz nagrodÄ™ za `feet_air_time` (zachÄ™ca do chodzenia)
3. Zmniejsz `action_scale` w konfiguracji (mniejsze, pÅ‚ynniejsze ruchy)

**Robot nie idzie do przodu:**
1. ZwiÄ™ksz `tracking_lin_vel` w nagrodach
2. SprawdÅº czy komenda prÄ™dkoÅ›ci jest rÃ³Å¼na od zera
3. Zmniejsz `stand_still` penalty

**Trening nie konwerguje:**
1. Zmniejsz `learning_rate`
2. ZwiÄ™ksz `num_envs` dla lepszej statystyki
3. SprawdÅº czy `normalize_observations=True`

---

## Transfer sim-to-real

Transfer sim-to-real to proces przenoszenia polityki wytrenowanej w symulacji do rzeczywistego robota.

### Kluczowe techniki:

#### 1. Domain Randomization

**ZAWSZE uÅ¼ywaj podczas treningu dla rzeczywistego robota!**

```bash
python learning/train_jax_ppo.py \
    --env_name G1JoystickFlatTerrain \
    --domain_randomization
```

Co jest randomizowane:
- Masa segmentÃ³w robota (Â±20%)
- WspÃ³Å‚czynniki tarcia (Â±50%)
- OpÃ³Åºnienia aktuatorÃ³w
- SiÅ‚y zakÅ‚Ã³ceÅ„ (wiatr, pchniÄ™cia)

#### 2. Dodaj szum do obserwacji

JuÅ¼ skonfigurowane w Å›rodowisku G1:

```python
# W konfiguracji G1
noise_config = {
    'level': 1.0,  # 0.0 = brak szumu, 1.0 = peÅ‚ny szum
    'scales': {
        'joint_pos': 0.03,    # Szum w odczytach pozycji stawÃ³w
        'joint_vel': 1.5,      # Szum w prÄ™dkoÅ›ciach
        'gravity': 0.05,       # Szum w odczytach orientacji
        'linvel': 0.1,         # Szum w prÄ™dkoÅ›ci liniowej
        'gyro': 0.2,           # Szum w Å¼yroskopie
    }
}
```

#### 3. Ograniczenia fizyczne

```python
# Ogranicz zakres stawÃ³w do bezpiecznych wartoÅ›ci
config.restricted_joint_range = True
# Ogranicz maksymalnÄ… prÄ™dkoÅ›Ä‡
config.action_scale = 0.3  # Zmniejsz dla bezpieczeÅ„stwa
```

### Procedura transferu:

1. **Trening w symulacji** (2-5M krokÃ³w z domain randomization)
2. **Walidacja w symulacji** (sprawdÅº odpornoÅ›Ä‡ na zakÅ‚Ã³cenia)
3. **Test w Å›rodowisku kontrolowanym** (robot na podwieszeniu/asekuracji)
4. **Stopniowe zwiÄ™kszanie swobody** (najpierw maÅ‚e ruchy, potem peÅ‚ny chÃ³d)
5. **Fine-tuning** (opcjonalnie dotrening na rzeczywistym robocie)

### Checklist przed testem na robocie:

- [ ] Model wytrenowany z `--domain_randomization`
- [ ] DziaÅ‚anie sprawdzone w symulacji z rÃ³Å¼nymi zaburzeniami
- [ ] Ograniczenia zakresu stawÃ³w wÅ‚Ä…czone
- [ ] Action scale ustawiony na bezpiecznÄ… wartoÅ›Ä‡ (â‰¤0.5)
- [ ] System awaryjnego zatrzymania przygotowany
- [ ] PrzestrzeÅ„ testowa zabezpieczona (materace, asekuracja)

---

## CzÄ™sto zadawane pytania

### Q: Jak dÅ‚ugo trwa trening?

**A**: ZaleÅ¼y od Å›rodowiska i sprzÄ™tu:
- Proste Å›rodowisko (CartPole): 1-5 minut
- G1 na pÅ‚askim terenie: 1-3 godziny
- ZÅ‚oÅ¼one manipulacje: 5-10 godzin

*Czasy dla GPU NVIDIA A100 z 4096 rÃ³wnolegÅ‚ymi Å›rodowiskami*

### Q: Ile pamiÄ™ci GPU potrzebujÄ™?

**A**: Orientacyjne wymagania:
- 8 GB: 1024 Å›rodowiska
- 16 GB: 2048-4096 Å›rodowiska
- 24 GB: 4096-8192 Å›rodowiska
- 40+ GB: >8192 Å›rodowiska

### Q: Czy mogÄ™ trenowaÄ‡ bez GPU?

**A**: Tak, ale bÄ™dzie BARDZO wolno (100-1000x wolniej). JAX moÅ¼e dziaÅ‚aÄ‡ na CPU:

```bash
# Zainstaluj JAX bez CUDA
pip install jax

# Zmniejsz liczbÄ™ Å›rodowisk
python learning/train_jax_ppo.py \
    --env_name G1JoystickFlatTerrain \
    --num_envs 64 \  # Zamiast 4096
    --num_timesteps 100000  # KrÃ³tszy trening
```

### Q: Jak wybraÄ‡ najlepszy checkpoint?

**A**: Nie zawsze ostatni checkpoint jest najlepszy! SprawdÅº:

```bash
# OdtwÃ³rz rÃ³Å¼ne checkpointy i porÃ³wnaj
python learning/train_jax_ppo.py \
    --env_name G1JoystickFlatTerrain \
    --play_only \
    --load_checkpoint_path logs/.../checkpoints/1000000 \
    --num_videos 5
```

Wybierz checkpoint z:
- NajwyÅ¼szÄ… Å›redniÄ… nagrodÄ… podczas ewaluacji
- Najbardziej stabilnym zachowaniem
- NajlepszÄ… odpornoÅ›ciÄ… na zakÅ‚Ã³cenia

### Q: Jak dostosowaÄ‡ Å›rodowisko do wÅ‚asnych potrzeb?

**A**: MoÅ¼esz modyfikowaÄ‡ konfiguracjÄ™:

```python
# Zobacz dostÄ™pne opcje
from mujoco_playground import registry
config = registry.get_default_config('G1JoystickFlatTerrain')
print(config)

# Modyfikuj i zapisz
config.ctrl_dt = 0.01  # CzÄ™stotliwoÅ›Ä‡ sterowania
config.episode_length = 2000  # DÅ‚ugoÅ›Ä‡ epizodu
# ... i uÅ¼yj w treningu z --playground_config_overrides
```

### Q: Gdzie znaleÅºÄ‡ wiÄ™cej przykÅ‚adÃ³w?

**A**: 
- Notebooki Jupyter w `learning/notebooks/`
- PrzykÅ‚adowe skrypty w `mujoco_playground/experimental/`
- Dokumentacja online: https://playground.mujoco.org/
- GitHub Issues: https://github.com/google-deepmind/mujoco_playground/issues

---

## Dodatkowe zasoby

### Polecane materiaÅ‚y do nauki:

1. **Uczenie ze wzmocnieniem**:
   - Spinning Up in Deep RL (OpenAI): https://spinningup.openai.com/
   - Sutton & Barto: "Reinforcement Learning: An Introduction"

2. **MuJoCo i symulacja**:
   - Dokumentacja MuJoCo: https://mujoco.readthedocs.io/
   - MJX Tutorial: https://mujoco.readthedocs.io/en/stable/mjx.html

3. **JAX**:
   - JAX Quickstart: https://jax.readthedocs.io/en/latest/quickstart.html
   - JAX Tutorial (Polski): [YouTube - JAX basics]

### SpoÅ‚ecznoÅ›Ä‡ i wsparcie:

- **Discord**: [MuJoCo Community Discord]
- **GitHub Discussions**: https://github.com/google-deepmind/mujoco_playground/discussions
- **Forum**: https://github.com/google-deepmind/mujoco/discussions

---

## Podsumowanie

Ten przewodnik powinien zapewniÄ‡ solidne podstawy do pracy z robotem Unitree G1 w MuJoCo Playground. PamiÄ™taj:

1. **Zacznij od prostych eksperymentÃ³w** - najpierw poznaj Å›rodowisko
2. **Eksperymentuj z parametrami** - ucz siÄ™ jak wpÅ‚ywajÄ… na zachowanie
3. **Zapisuj wszystko** - dokumentuj swoje eksperymenty
4. **Testuj stopniowo** - od symulacji do rzeczywistoÅ›ci maÅ‚ymi krokami
5. **BezpieczeÅ„stwo przede wszystkim** - szczegÃ³lnie przy pracy z fizycznym robotem

**Powodzenia w pracy z robotem G1!** ğŸ¤–

---

*Dokument przygotowany dla studentÃ³w Politechniki Rzeszowskiej*
*Ostatnia aktualizacja: 2025-02-10*
